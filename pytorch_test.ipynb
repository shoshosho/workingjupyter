{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f45cb3cb-c529-4c96-874d-b8939e0153b0"
    }
   },
   "outputs": [],
   "source": [
    "# torchの基本的な書き方\n",
    "# import \n",
    "# data与える\n",
    "# modelを定義\n",
    "# criterion, optimizerを定義\n",
    "# for epochで　pred, loss, zero optimizer, 逆伝搬, updateを記述\n",
    "\n",
    "# 正則化の話はどこに入れ込めるのか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "39bb5329-a0c3-4897-ad7c-30253bdcdb8a"
    }
   },
   "outputs": [],
   "source": [
    "# python -Vだと2.7を指すが、ここだと大丈夫\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9b412922-02ed-4e33-9196-c9db235a4dcd"
    }
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    PATH = \"/home/vmcntkadmin003/notebooks/CNTKSeminarPrac/CNN/dataset/MNIST_dataset.npz\"\n",
    "    mnist_data = np.load(PATH)\n",
    "    x_train = mnist_data[\"x_train\"]\n",
    "    x_test = mnist_data[\"x_test\"]\n",
    "    y_train = mnist_data[\"y_train\"]\n",
    "    y_test = mnist_data[\"y_test\"]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2bbe9b80-984d-44ca-b31c-63a01a8e1f82"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_mnist()\n",
    "print(\"x_train.shape:%s, x_test.shape:%s, y_train.shape:%s, y_test.shape:%s\"\n",
    "      % (x_train.shape, x_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "62548f5a-a850-439b-98f1-5db11af6cec1"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "16844bf2-2f0b-4ca0-9f6d-4e0924dfbdbf"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "    # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "    # Tensors.\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6d92b948-a032-4356-9464-abdcad214f06"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "69626e56-1d7c-4010-8eeb-692f89bedaf6"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplolib inline\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "\n",
    "# our model for the forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w=\", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    print(\"MSE=\", l_sum / 3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "\n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5d7e839b-e5e3-4e3e-9eb9-6deec0b535c5"
    }
   },
   "outputs": [],
   "source": [
    "# torchの基本的な書き方\n",
    "# import \n",
    "# data与える\n",
    "# modelを定義\n",
    "# criterion, optimizerを定義\n",
    "# for epochで　pred, loss, zero optimizer, 逆伝搬, updateを記述\n",
    "\n",
    "# 正則化の話はどこに入れ込めるのか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "19f7eb09-30b0-459a-acd7-1bc812a58026"
    }
   },
   "outputs": [],
   "source": [
    "# 線形モデル実装ケース\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\",  4, model(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6585435b-54f8-48a0-b14b-55072873d73e"
    }
   },
   "outputs": [],
   "source": [
    "# 線形モデル実装ケース\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "model = Model()\n",
    "\n",
    "# criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "# hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "# y_pred = model(hour_var)\n",
    "# print(\"predict (after training)\",  4, model(hour_var).data[0][0])\n",
    "\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b3fe2abc-5fff-4a66-b772-ee01ab133f13"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    " \n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    " \n",
    "class Model(torch.nn.Module):\n",
    " \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for epoch in range(1000):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    " \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "#     print(epoch, loss.data[0])\n",
    " \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b73ca1cd-acfc-4c06-90d2-a9b67a0a5e9e"
    }
   },
   "outputs": [],
   "source": [
    "model(Variable(torch.Tensor([[1.0],[7.0]]))).data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cd397c9a-2795-4d70-8017-88ce42b11dd2"
    }
   },
   "outputs": [],
   "source": [
    "# mnist\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# Training settings\n",
    "batch_size = 64\n",
    " \n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    " \n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    " \n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        super().__init__() #python3だと省略可能\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    " \n",
    " #さらに親クラスを継承したmodelクラスを作成する\n",
    "model = Net()\n",
    " \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "\n",
    "def train(epoch):model\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # 前回計算したgradをゼロクリアする\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    " \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    " \n",
    " \n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bc5e537f-14fe-4ae8-b656-32d813608cca"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        super().__init__() #python3だと省略可能\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "afc4f7f3-812b-49ce-9e4c-7580e86a2fcd"
    }
   },
   "outputs": [],
   "source": [
    "# model.parameters()\n",
    "nn.Module.parameters(\"self\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b1854efc-acae-43ff-9f68-df0e77579419"
    }
   },
   "outputs": [],
   "source": [
    "optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "53714f3a-5aad-4263-b69d-2938516359dc"
    }
   },
   "outputs": [],
   "source": [
    "# 7章、糖尿病を10層以上のdeepnetで予測してみる\n",
    "# 書き方不慣れなので、大事なとこはコピペ\n",
    "# logistic回帰\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    " \n",
    "xy = np.loadtxt('/home/vmcntkadmin003/notebooks/pytorch/PyTorchZeroToAll/data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    " \n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    " \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x): \n",
    "        \"\"\" \n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    " \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    " \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if epoch%10 ==0:\n",
    "        print(epoch, loss.data[0])\n",
    " \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルできてるので予測すればOK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f661459c-d1ce-4058-877e-d971576f6b2a"
    }
   },
   "outputs": [],
   "source": [
    "# とりあえず使い方\n",
    "# prediction = model(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "760db136-64ee-43a2-86e5-8081bd27d893"
    }
   },
   "outputs": [],
   "source": [
    "# 8章 data loader\n",
    "# エポック、バッチ、イテレーション\n",
    "\n",
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('/home/vmcntkadmin003/notebooks/pytorch/PyTorchZeroToAll/data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "lis = []\n",
    "for epoch in range(20):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data[0])\n",
    "        lis.append(loss.data[0])\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "37bc8e1d-cbf9-43d7-a224-053dd67396c7"
    }
   },
   "outputs": [],
   "source": [
    "# metricsの変化グラフ\n",
    "# きもい\n",
    "# うまくいってない\n",
    "# epoch, batch, iterationの組合せがいけてない or 方法が間違ってる\n",
    "\n",
    "plt.plot(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a103e3c6-5b6a-45a0-b931-397f59e11b62"
    }
   },
   "outputs": [],
   "source": [
    "# 宿題\n",
    "# kaggleタイタニックのdataloaderを作る\n",
    "# タイタニックの判別器を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "741636b0-fbcb-45f4-ab22-be9b05035e0d"
    }
   },
   "outputs": [],
   "source": [
    "# 9章 softmax - 例\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Cross entropy example\n",
    "import numpy as np\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n",
    "print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n",
    "\n",
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n",
    "\n",
    "print(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\n",
    "print(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                 [1.1, 0.1, 0.2],\n",
    "                                 [0.2, 2.1, 0.1]]))\n",
    "\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                 [0.2, 0.3, 0.5],\n",
    "                                 [0.2, 0.2, 0.5]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1af12b10-09af-44b1-bb1a-65230f2067d5"
    }
   },
   "outputs": [],
   "source": [
    "# 9章 softmax to mnist\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    tra = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            tra.append(loss.data[0])\n",
    "    return tra\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lis = []\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    lis.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return lis\n",
    "\n",
    "li = []\n",
    "tr = []\n",
    "for epoch in range(1, 10):\n",
    "    tra = train(epoch)\n",
    "    lis = test()\n",
    "    li.append(lis)\n",
    "    tr.append(tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "44257336-bc93-4787-80e4-1048bee77810"
    }
   },
   "outputs": [],
   "source": [
    "# train_error\n",
    "# 挙動はきもいし、データは大量にいるし、なぜうまくいくかが説明できないけど、とにかく良くなっている\n",
    "flat_list = [item for sublist in tr for item in sublist]\n",
    "plt.plot(flat_list)\n",
    "\n",
    "num_epoch = 10\n",
    "iter_size = len(flat_list)/ num_epoch\n",
    "iter_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch毎のtest_error\n",
    "# trainと大体同じ動きをする\n",
    "# 深くするだけの価値が(もしかしたら)あるかもしれない\n",
    "plt.plot(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10章\n",
    "# CNNの基礎\n",
    "\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            tra.append(loss.data[0])\n",
    "    return tra\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    ts.append(test_loss)\n",
    "    return ts\n",
    "\n",
    "\n",
    "tra,ts = [],[]\n",
    "for epoch in range(1, 10):\n",
    "    tr = train(epoch)\n",
    "    ts = test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これだけでうまくいくのが不思議な感じだけど、まあうまくいってるんだろう\n",
    "# よりノーマルな構成と比較して滑らかに、まっすぐmetricsが落ちていく感じ\n",
    "# →より良いと言ってよさそう\n",
    "plt.plot(ts)\n",
    "# ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = [item for sublist in tr for item in sublist]\n",
    "# plt.plot(flat_list)\n",
    "plt.plot(tr)\n",
    "# tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 宿題\n",
    "# 層の深いCNNを作る(conv1, maxpool1, conv2, maxpool2, conv3, maxpool3, locally connected, fully connexted*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11章\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            tr.append(loss.data[0])\n",
    "    return tr\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    ts.append(test_loss)\n",
    "    return ts\n",
    "\n",
    "\n",
    "tr, ts = [], []\n",
    "start = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    t = train(epoch)\n",
    "    s = test()\n",
    "    tr.append(t)\n",
    "    ts.append(s)\n",
    "end = time.time()\n",
    "\n",
    "print(\"taken_time is:\", end-start,\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# さすがに学習は結構遅くなってくる\n",
    "# ここで詰まったので、もう１回流したい\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 宿題\n",
    "# full-inception v3/v4の実装\n",
    "# https://research.googleblog.com/2016/08/improving-inception-and-image.html \n",
    "\n",
    "# res-netの実装\n",
    "# Try to implement from scratch \n",
    "# Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n",
    "#  Identity Mappings in Deep Residual Networks: https://arxiv.org/abs/1603.05027 \n",
    "# http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf  \n",
    "    \n",
    "# dense-netの実装\n",
    "# https://arxiv.org/abs/1608.06993  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-1 RNNの基礎\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "# (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "hidden = (Variable(torch.randn(1, 1, 2)))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"one input size\", one.size(), \"out size\", out.size())\n",
    "\n",
    "# We can do the whole at once\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())\n",
    "\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "# rank = (3, 5, 4)\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n",
    "\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2)\n",
    "cell = nn.RNN(input_size=4, hidden_size=2)\n",
    "\n",
    "# print(inputs)\n",
    "\n",
    "# The given dimensions dim0 and dim1 are swapped.\n",
    "# inputs = inputs.transpose(3, dim1=1, dim2=2)\n",
    "inputs = torch.transpose(inputs, dim0=0, dim1=1)\n",
    "# Propagate input through RNN\n",
    "# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n",
    "# S x B x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-2 helloを覚えさせる\n",
    "# Lab 12 RNN\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "#            0    1    2    3    4\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
    "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
    "                  [0, 1, 0, 0, 0],  # 1\n",
    "                  [0, 0, 1, 0, 0],  # 2\n",
    "                  [0, 0, 0, 1, 0],  # 3\n",
    "                  [0, 0, 0, 0, 1]]  # 4\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 1  # One by one\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        # Reshape input (batch first)\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # hidden: (batch, num_layers * num_directions, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and cell states\n",
    "        # (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "        return Variable(torch.zeros(batch_size, num_layers, hidden_size))\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    sys.stdout.write(\"predicted string: \")\n",
    "    for input, label in zip(inputs, labels):\n",
    "        # print(input.size(), label.size())\n",
    "        hidden, output = model(hidden, input)\n",
    "        val, idx = output.max(1)\n",
    "        sys.stdout.write(idx2char[idx.data[0]])\n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Learning finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-3 hello_cnn_seq\n",
    "\n",
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            x.size(0), self.num_layers, self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (batch, num_layers * num_directions, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        return out.view(-1, num_classes)\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-4 hello_rnn_emb\n",
    "\n",
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.LongTensor(x_data))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5\n",
    "embedding_size = 10  # embedding size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,\n",
    "                          hidden_size=5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            x.size(0), num_layers, hidden_size))\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (batch, num_layers * num_directions, hidden_size)\n",
    "        out, _ = self.rnn(emb, h_0)\n",
    "        return self.fc(out.view(-1, num_classes))\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = model(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 宿題\n",
    "# Implement using a softmax classifier for ‘hihell’ to ‘ihello’ \n",
    "\n",
    "# Combine RNN+Linear\n",
    "\n",
    "# Combine RNN+Linear using embedding\n",
    "\n",
    "# RNNの実装\n",
    "# Hint: http://blog.varunajayasiri.com/numpy_lstm.html \n",
    "\n",
    "#Implement GRU\n",
    "# http://blog.varunajayasiri.com/numpy_lstm.html \n",
    "# http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-1 name-classification basic\n",
    "\n",
    "# Original code is from\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PyTorchZeroToAll.name_dataset import NameDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_CHARS = 128  # ASCII\n",
    "N_CLASSES = 18\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # input:  B x S  -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        print(\"  input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        print(\"  embedding\", embedded.size())\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(\"  gru hidden output\", hidden.size())\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        print(\"  fc output\", fc_output.size())\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)\n",
    "\n",
    "# Help functions\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    names = ['adylov', 'solan', 'hard', 'san']\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
    "\n",
    "    for name in names:\n",
    "        arr, _ = str2ascii_arr(name)\n",
    "        inp = Variable(torch.LongTensor([arr]))\n",
    "        out = classifier(inp)\n",
    "        print(\"in\", inp.size(), \"out\", out.size())\n",
    "\n",
    "\n",
    "    inputs = make_variables(names)\n",
    "    out = classifier(inputs)\n",
    "    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-2 rnn-classification\n",
    "\n",
    "# Original code is from https://github.com/spro/practical-pytorch\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PyTorchZeroToAll.name_dataset import NameDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "\n",
    "test_dataset = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "N_COUNTRIES = len(train_dataset.get_countries())\n",
    "print(N_COUNTRIES, \"countries\")\n",
    "N_CHARS = 128  # ASCII\n",
    "\n",
    "\n",
    "# Some utility functions\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def create_variable(tensor):\n",
    "    # Do cuda() before wrapping with variable\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "# pad sequences and sort the tensor\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # Sort tensors by their length\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "    # Also sort the target (countries) in the same order\n",
    "    target = countries2tensor(countries)\n",
    "    if len(countries):\n",
    "        target = target[perm_idx]\n",
    "\n",
    "    # Return variables\n",
    "    # DataParallel requires everything to be a Variable\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "\n",
    "\n",
    "# Create necessary variables, lengths, and target\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def countries2tensor(countries):\n",
    "    country_ids = [train_dataset.get_country_id(\n",
    "        country) for country in countries]\n",
    "    return torch.LongTensor(country_ids)\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        # input shape: B x S (input size)\n",
    "        # transpose to make S(sequence) x B (batch)\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Pack them up nicely\n",
    "        gru_input = pack_padded_sequence(\n",
    "            embedded, seq_lengths.data.cpu().numpy())\n",
    "\n",
    "        # To compact weights again call flatten_parameters().\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                             batch_size, self.hidden_size)\n",
    "        return create_variable(hidden)\n",
    "\n",
    "\n",
    "# Train cycle\n",
    "def train():\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (names, countries) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
    "                time_since(start), epoch,  i *\n",
    "                len(names), len(train_loader.dataset),\n",
    "                100. * i * len(names) / len(train_loader.dataset),\n",
    "                total_loss / i * len(names)))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Testing cycle\n",
    "def test(name=None):\n",
    "    # Predict for a given name\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        country_id = pred.cpu().numpy()[0][0]\n",
    "        print(name, \"is\", train_dataset.get_country(country_id))\n",
    "        return\n",
    "\n",
    "    print(\"evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "\n",
    "    for names, countries in test_loader:\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
    "        classifier = nn.DataParallel(classifier)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        classifier.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        train()\n",
    "\n",
    "        # Testing\n",
    "        test()\n",
    "\n",
    "        # Testing several samples\n",
    "        test(\"Sung\")\n",
    "        test(\"Jungwoo\")\n",
    "        test(\"Soojin\")\n",
    "        test(\"Nako\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-3 char-rnn encoder-decoderの話っぽい\n",
    "\n",
    "# https://github.com/spro/practical-pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PyTorchZeroToAll.text_loader import TextDataset\n",
    "\n",
    "hidden_size = 100\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "n_epochs = 100\n",
    "n_characters = 128  # ASCII\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # This runs this one step at a time\n",
    "    # It's extremely slow, and please do not use in practice.\n",
    "    # We need to use (1) batch and (2) data parallelism\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input.view(1, -1))  # S(=1) x I\n",
    "        embed = embed.view(1, 1, -1)  # S(=1) x B(=1) x I (embedding size)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.linear(output.view(1, -1))  # S(=1) x I\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "\n",
    "        return Variable(hidden)\n",
    "\n",
    "\n",
    "def str2tensor(string):\n",
    "    tensor = [ord(c) for c in string]\n",
    "    tensor = torch.LongTensor(tensor)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = str2tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "        inp = str2tensor(predicted_char)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "# Train for a given src and target\n",
    "# It feeds single string to demonstrate seq2seq\n",
    "# It's extremely slow, and we need to use (1) batch and (2) data parallelism\n",
    "# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n",
    "\n",
    "\n",
    "def train_teacher_forching(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(input)):\n",
    "        output, hidden = decoder(input[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(input)\n",
    "\n",
    "\n",
    "def train(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_in = input[0]\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(input)):\n",
    "        output, hidden = decoder(decoder_in, hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "        decoder_in = output.max(1)[1]\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(input)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "    if torch.cuda.is_available():\n",
    "        decoder.cuda()\n",
    "\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(dataset=TextDataset(),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "    print(\"Training for %d epochs...\" % n_epochs)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for i, (lines, _) in enumerate(train_loader):\n",
    "            loss = train(lines[0])  # Batch size is 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[(%d %d%%) loss: %.4f]' %\n",
    "                      (epoch, epoch / n_epochs * 100, loss))\n",
    "                print(generate(decoder, 'Wh', 100), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 13-4 pac-pad\n",
    "\n",
    "# Original source from\n",
    "# https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "\n",
    "# make model\n",
    "embedding_size = 3\n",
    "embed = nn.Embedding(len(vocab), embedding_size)\n",
    "lstm = nn.LSTM(embedding_size, 5)\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "print(\"vectorized_seqs\", vectorized_seqs)\n",
    "\n",
    "print([x for x in map(len, vectorized_seqs)])\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n",
    "\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros(\n",
    "    (len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "print(\"seq_tensor\", seq_tensor)\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "print(\"seq_tensor after sorting\", seq_tensor)\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
    "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# embed your sequences\n",
    "embeded_seq_tensor = embed(seq_tensor)\n",
    "print(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(\n",
    "    embeded_seq_tensor, seq_lengths.cpu().numpy())\n",
    "\n",
    "# throw them through your LSTM (remember to give batch_first=True here if\n",
    "# you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print(\"Lstm output\", output.size(), output.data)\n",
    "\n",
    "# Or if you just want the final hidden state?\n",
    "print(\"Last output\", ht[-1].size(), ht[-1].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14-1 seq to seq\n",
    "\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PyTorchZeroToAll.text_loader import TextDataset\n",
    "import seq2seq_models as sm\n",
    "from seq2seq_models import str2tensor, EOS_token, SOS_token\n",
    "\n",
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 1\n",
    "N_EPOCH = 100\n",
    "N_CHARS = 128  # ASCII\n",
    "\n",
    "\n",
    "# Simple test to show how our network works\n",
    "def test():\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    word_input = str2tensor('hello')\n",
    "    encoder_outputs, encoder_hidden = encoder(word_input, encoder_hidden)\n",
    "    print(encoder_outputs)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    word_target = str2tensor('pytorch')\n",
    "    for c in range(len(word_target)):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            word_target[c], decoder_hidden)\n",
    "        print(decoder_output.size(), decoder_hidden.size())\n",
    "\n",
    "\n",
    "# Train for a given src and target\n",
    "# To demonstrate seq2seq, We don't handle batch in the code,\n",
    "# and our encoder runs this one step at a time\n",
    "# It's extremely slow, and please do not use in practice.\n",
    "# We need to use (1) batch and (2) data parallelism\n",
    "# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n",
    "def train(src, target):\n",
    "    src_var = str2tensor(src)\n",
    "    target_var = str2tensor(target, eos=True)  # Add the EOS token\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
    "\n",
    "    hidden = encoder_hidden\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(len(target_var)):\n",
    "        # First, we feed SOS\n",
    "        # Others, we use teacher forcing\n",
    "        token = target_var[c - 1] if c else str2tensor(SOS_token)\n",
    "        output, hidden = decoder(token, hidden)\n",
    "        loss += criterion(output, target_var[c])\n",
    "\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(target_var)\n",
    "\n",
    "\n",
    "# Translate the given input\n",
    "def translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n",
    "    input_var = str2tensor(enc_input)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "\n",
    "    hidden = encoder_hidden\n",
    "\n",
    "    predicted = ''\n",
    "    dec_input = str2tensor(SOS_token)\n",
    "    for c in range(predict_len):\n",
    "        output, hidden = decoder(dec_input, hidden)\n",
    "\n",
    "        # Sample from the network as a multi nominal distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Stop at the EOS\n",
    "        if top_i is EOS_token:\n",
    "            break\n",
    "\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "\n",
    "        dec_input = str2tensor(predicted_char)\n",
    "\n",
    "    return enc_input, predicted\n",
    "\n",
    "\n",
    "encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
    "decoder = sm.DecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    decoder.cuda()\n",
    "    encoder.cuda()\n",
    "print(encoder, decoder)\n",
    "test()\n",
    "\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=TextDataset(),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "print(\"Training for %d epochs...\" % N_EPOCH)\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    # Get srcs and targets from data loader\n",
    "    for i, (srcs, targets) in enumerate(train_loader):\n",
    "        train_loss = train(srcs[0], targets[0])  # Batch is 1\n",
    "\n",
    "        if i % 100 is 0:\n",
    "            print('[(%d %d%%) %.4f]' %\n",
    "                  (epoch, epoch / N_EPOCH * 100, train_loss))\n",
    "            print(translate(srcs[0]), '\\n')\n",
    "            print(translate(), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 14-2 seq to seq with attention\n",
    "\n",
    "# Original code from\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from PyTorchZeroToAll.text_loader import TextDataset\n",
    "import seq2seq_models as sm\n",
    "from seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token\n",
    "\n",
    "\n",
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 1\n",
    "N_EPOCH = 100\n",
    "N_CHARS = 128  # ASCII\n",
    "HIDDEN_SIZE = N_CHARS\n",
    "\n",
    "\n",
    "# Simple test to show how our train works\n",
    "def test():\n",
    "    encoder_test = sm.EncoderRNN(10, 10, 2)\n",
    "    decoder_test = sm.AttnDecoderRNN(10, 10, 2)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        encoder_test.cuda()\n",
    "        decoder_test.cuda()\n",
    "\n",
    "    encoder_hidden = encoder_test.init_hidden()\n",
    "    word_input = cuda_variable(torch.LongTensor([1, 2, 3]))\n",
    "    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "    print(encoder_outputs.size())\n",
    "\n",
    "    word_target = cuda_variable(torch.LongTensor([1, 2, 3]))\n",
    "    decoder_attns = torch.zeros(1, 3, 3)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for c in range(len(word_target)):\n",
    "        decoder_output, decoder_hidden, decoder_attn = \\\n",
    "            decoder_test(word_target[c],\n",
    "                         decoder_hidden, encoder_outputs)\n",
    "        print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
    "        decoder_attns[0, c] = decoder_attn.squeeze(0).cpu().data\n",
    "\n",
    "\n",
    "# Train for a given src and target\n",
    "# To demonstrate seq2seq, We don't handle batch in the code,\n",
    "# and our encoder runs this one step at a time\n",
    "# It's extremely slow, and please do not use in practice.\n",
    "# We need to use (1) batch and (2) data parallelism\n",
    "# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n",
    "def train(src, target):\n",
    "    loss = 0\n",
    "\n",
    "    src_var = str2tensor(src)\n",
    "    target_var = str2tensor(target, eos=True)  # Add the EOS token\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
    "\n",
    "    hidden = encoder_hidden\n",
    "\n",
    "    for c in range(len(target_var)):\n",
    "        # First, we feed SOS. Others, we use teacher forcing.\n",
    "        token = target_var[c - 1] if c else str2tensor(SOS_token)\n",
    "        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n",
    "        loss += criterion(output, target_var[c])\n",
    "\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(target_var)\n",
    "\n",
    "\n",
    "# Translate the given input\n",
    "def translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n",
    "    input_var = str2tensor(enc_input)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "\n",
    "    hidden = encoder_hidden\n",
    "\n",
    "    predicted = ''\n",
    "    dec_input = str2tensor(SOS_token)\n",
    "    attentions = []\n",
    "    for c in range(predict_len):\n",
    "        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n",
    "        # Sample from the network as a multi nominal distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n",
    "\n",
    "        # Stop at the EOS\n",
    "        if top_i is EOS_token:\n",
    "            break\n",
    "\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "\n",
    "        dec_input = str2tensor(predicted_char)\n",
    "\n",
    "    return predicted, attentions\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
    "    decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        decoder.cuda()\n",
    "        encoder.cuda()\n",
    "    print(encoder, decoder)\n",
    "    # test()\n",
    "\n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(dataset=TextDataset(),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2)\n",
    "\n",
    "    print(\"Training for %d epochs...\" % N_EPOCH)\n",
    "    for epoch in range(1, N_EPOCH + 1):\n",
    "        # Get srcs and targets from data loader\n",
    "        for i, (srcs, targets) in enumerate(train_loader):\n",
    "            train_loss = train(srcs[0], targets[0])\n",
    "\n",
    "            if i % 1000 is 0:\n",
    "                print('[(%d/%d %d%%) %.4f]' %\n",
    "                      (epoch, N_EPOCH, i * len(srcs) * 100 / len(train_loader), train_loss))\n",
    "                output, _ = translate(srcs[0])\n",
    "                print(srcs[0], output, '\\n')\n",
    "\n",
    "                output, attentions = translate()\n",
    "                print('thisissungkim.iloveyou.', output, '\\n')\n",
    "\n",
    "        # plt.matshow(attentions)\n",
    "        # plt.show()\n",
    "        # print(attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
